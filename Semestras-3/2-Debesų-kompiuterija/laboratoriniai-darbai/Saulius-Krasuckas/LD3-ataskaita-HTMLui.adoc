= {nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}Vilniaus Gedimino technikos universitetas

[.text-center]
== Elektronikos fakultetas

=== Kompiuterijos ir ryšių technologijų katedra

{nbsp}

{nbsp}

{nbsp}

{nbsp}

{nbsp}

{nbsp}

=== Debesų kompiuterija
Modulis ELKRM17304

[.text-center]
== Susipažinimas su Kubernetes platforma

Laboratorinio darbo nr. 3 ataskaita

{nbsp}

{nbsp}

{nbsp}

{nbsp}

{nbsp}

{nbsp}

[.text-right]
**Atliko:** TETfm-20 grupės magistrantas +
                       Saulius Krasuckas +
**Tikrino:** lekt. dr. Liudas Duoba

{nbsp}

{nbsp}

{nbsp}

{nbsp}

{nbsp}

{nbsp}

{nbsp}

VILNIUS, 2022

<<<



{nbsp}

[.text-center]
==== Laboratorinis darbas nr. 3

[.text-center]
== Susipažinimas su Kubernetes platforma


{nbsp}

[.text-center]
=== Darbo tikslas

[.text-left]
* Išbandyti `Kubernetes` platformą.
* Atlikti pagrindinius veiksmus su `Kubernetes` resursais.


[.text-center]
=== Darbo eiga

[.text-left]

Puslapyje *https://www.katacoda.com/courses/kubernetes[Learn Kubernetes using Interactive Browser-Based Labs | Katacoda]* susikūriau prisijungimą pasinaudodamas "_Log In with Github_" metodu ir autorizuodamas savo akademinę GitHub paskyrą.


[.text-left]
=== Užduotis nr. 1: "*https://www.katacoda.com/courses/kubernetes/launch-single-node-cluster[Launch Single Node Kubernetes Cluster]*"

==== Įvadas

_Minikube_ -- įrankis pradėti naudotis Kubernetes lokaliai, bet produkcinio tinklo.
Jis virtualioje mašinoje (VM) startuoja vienamazgį (angl. _single-node_) Kubernetes klasterį ir tinka asmeniniam kompiuteriui.
Skirtas naudotojams, siekiantiems išmėginti Kubernetes ar net vykdyti kasdienį sistemų kūrimą Kubernetes pagrindu.

==== Žingsnis 1.1: *`Minikube` startas*

    - patikrinu versiją:
+++
<script id="asciicast-452640" src="https://asciinema.org/a/452640.js" async></script>
+++


    - startuoju VM su `Kubernetes` klasteriu:
+++
<script id="asciicast-452643" src="https://asciinema.org/a/452643.js" async></script>
+++


==== Žingsnis 1.2: *_Kubernetes_ klasterio informacija*

    - tikrinu klasterio būseną:
+++
<script id="asciicast-452671" src="https://asciinema.org/a/452671.js" async></script>
+++


    - klasterio mazgų sąrašas:
+++
<script id="asciicast-452686" src="https://asciinema.org/a/452686.js" async></script>
+++


==== Žingsnis 1.3: *diegiame konteinerį klasteryje*

    - konteinerio diegimas iš atvaizdo:
+
+++
<script id="asciicast-452688" src="https://asciinema.org/a/452688.js" async></script>
+++
Čia vaizdo įraše padariau klaidą įterpdamas vieną papildomą raidę: `kataco**n**da`.
Praktikoje vėliau tai pataisiau, tik nebeįrašiau:
+
----
$ kubectl create deployment first-deployment --image=katacoda/docker-http-server
deployment.apps/first-deployment created
----


    - tikrinu diegimo būseną:
+++
<script id="asciicast-452708" src="https://asciinema.org/a/452708.js" async></script>
+++


    - paviešinu konteinerį tinkle:
+++
<script id="asciicast-452709" src="https://asciinema.org/a/452709.js" async></script>
+++


    - susirandu alokuotą TCP-portą ir vykdome HTTP-užklausą:
+++
<script id="asciicast-452711" src="https://asciinema.org/a/452711.js" async></script>
+++


==== Žingsnis 1.4: *_Kubernetes Dashboard_ sąsaja (web-UI)*

    - įgalinu _Minicube_ priedą _Dashboard_:
+++
<script id="asciicast-452714" src="https://asciinema.org/a/452714.js" async></script>
+++


    - diegiu _Kubernetes Dashboard_ pagal duotą YAML šabloną:
+++
<script id="asciicast-452718" src="https://asciinema.org/a/452718.js" async></script>
+++


    - patikrinu šablono turinį:
+
----
$ ls -l /opt/kubernetes-dashboard.yaml
-rw-r--r-- 1 root root 588 Mar  8  2020 /opt/kubernetes-dashboard.yaml

$ cat /opt/kubernetes-dashboard.yaml
apiVersion: v1
kind: Namespace
metadata:
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/minikube-addons: dashboard
  name: kubernetes-dashboard
  selfLink: /api/v1/namespaces/kubernetes-dashboard
spec:
  finalizers:
  - kubernetes
status:
  phase: Active
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: kubernetes-dashboard
  name: kubernetes-dashboard-katacoda
  namespace: kubernetes-dashboard
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 9090
    nodePort: 30000
  selector:
    k8s-app: kubernetes-dashboard
  type: NodePort
----

    - stebiu _Dashboard_ konteinerių startą:
+++
<script id="asciicast-452725" src="https://asciinema.org/a/452725.js" async></script>
+++


    - tikrinu web-UI sąsają tiesiogiai:  +
      https://2886795274-30000-cykoria04.environments.katacoda.com/
      
      ** klasterio apžvalga:
+
image::https://user-images.githubusercontent.com/74717106/149772492-a72b5b07-9c09-463d-885a-3c4b81b31ff5.png[]
+
{nbsp}

      ** vardų srities apkrovos apžvalga:
+
image::https://user-images.githubusercontent.com/74717106/149772830-d20b2b96-3d10-432b-9d8a-78e34f04c4bc.png[]
+
{nbsp}

      ** bandomojo diegimo būsena:
+
image::https://user-images.githubusercontent.com/74717106/149774966-f3c803b3-7b9e-489d-9b82-a23e78d2c663.png[]
+
{nbsp}

      ** jo „ankšties“ būsena:
+
image::https://user-images.githubusercontent.com/74717106/149775048-a056fe1e-126f-4371-a9f9-88859feb2f34.png[]
+
{nbsp}

      ** paslaugų būsena, apkrovos balansavimas:
+
image::https://user-images.githubusercontent.com/74717106/149773732-aaf5f1c9-3c28-44e1-8fc6-05c5f44bf709.png[]
---
image::https://user-images.githubusercontent.com/74717106/149773955-ef7a3c7a-6826-4ca5-9723-f40e949fe007.png[]
---
image::https://user-images.githubusercontent.com/74717106/149774143-436458fd-7075-48cd-bcd2-21c7f464f4ba.png[]
+
{nbsp}

      ** vardų srities konfigūracija ir talpinimas:
+
image::https://user-images.githubusercontent.com/74717106/149774278-d7afe893-5549-47e7-a9fa-d3f51b425ab7.png[]
---
image::https://user-images.githubusercontent.com/74717106/149774439-804af510-6baa-4663-8037-56476357ddc9.png[]
+
{nbsp}


==== Suvestinė nr. 1:

    - Panaudojau `minikube` bei `kubectl` komandas (jų subkomandas) ir:
    
      . startavau vieno mazgo Kubernetes miniklasterį;  +
        (atskiroje VM, pasak gido)
      . patikrinau klasterio būseną: veikiantis;
      . sukūriau konteinerį pagal `katacoda/docker-http-server` atvaizdą;  +
        (tik vaizdo įraše padariau klaidą įterpdamas vieną papildomą raidę: `kataco**n**da`)
      . patikrinau diegimo „ankštį“: ji susikūrė konteinerį ir veikia;
      . paviešinau konteinerinę paslaugą tinkle atskiru `31900/TCP` portu;
      . prisijungiau šiuo portu su `curl` ir patikrinau paslaugos būseną: veikia;
      . įdiegiau ir startavau _Minicube_ priedą -- Web sąsają _Dashboard_
      . bei patikrinau klasterio būseną joje naudodamasis savo naršykle.  +
        (Nuoroda Web prisijungimui pateikė pats _katacoda.com_ gidas)

    - _Dashboard_ interfeisas _Overview_ skiltyje pasirenka `default` vardų sritį (_Namespace_):
      . joje nematyti savo paties „ankščių“ (_Pods_):  +
    `kubernetes-dashboard-79d9cd965-7f5pb`,  +
    `dashboard-metrics-scraper-7b64584c5c-7x46c`
      . pastarosios tampa matomos pasirinkus `All namespaces` vardų sritį.
    
<<<


[.text-left]
=== Užduotis nr. 2: "*https://www.katacoda.com/courses/kubernetes/kubectl-run-containers[Deploy Containers Using Kubectl]*"

==== Įvadas

Mokinsimės _Kubectl_ pagalba kurti ir startuoti įdiegimus, replikavimo valdiklius ir viešinti juos kaip paslaugas.
Čia nenaudosime YAML apibrėžčių.
Šis būdas klasteryje įgalina sparčiai pradėti konteinerius kūrimą ir jų vykdymą.


==== Žingsnis 2.1: *startuojame Kubernetes klasterį*

    - startuojame klasterį ir įgaliname Kubectl CLI:
+++
<script id="asciicast-462314" src="https://asciinema.org/a/462314.js" async></script>
+++


    - patikriname mazgo būseną:
+++
<script id="asciicast-462317" src="https://asciinema.org/a/462317.js" async></script>
+++


==== Žingsnis 2.2: *vykdome `kubectl` su `run`*
  
    - sukuriame įdiegimą ir startuojame jo „ankštis“ bei konteinerius:
+++
<script id="asciicast-462319" src="https://asciinema.org/a/462319.js" async></script>
+++


    - tikriname įdiegimų būsenas:
+++
<script id="asciicast-462320" src="https://asciinema.org/a/462320.js" async></script>
+++


    - tikriname išsamų įdiegimo aprašą:
+++
<script id="asciicast-462321" src="https://asciinema.org/a/462321.js" async></script>
+++


==== Žingsnis 2.3: *vykdome `kubectl` su `expose`*
  
    - sukuriame paslaugą paviešindami konkretų konteinerio portą:
+++
<script id="asciicast-462325" src="https://asciinema.org/a/462325.js" async></script>
+++


    - patikriname paslaugos veikimą:
+++
<script id="asciicast-462326" src="https://asciinema.org/a/462326.js" async></script>
+++


==== Žingsnis 2.4: *vykdome `kubectl` su `run`+`expose` iškart*
  
    - sukuriame naują įdiegimą ir paviešiname naują paslaugą kitu portu vienu ypu, kitu būdu:
+++
<script id="asciicast-462331" src="https://asciinema.org/a/462331.js" async></script>
+++


    - patikriname naujos paslaugos veikimą:
+++
<script id="asciicast-462333" src="https://asciinema.org/a/462333.js" async></script>
+++


    - tikriname, ar naujas portas tikrai neatsirado paslaugų sąraše:
+++
<script id="asciicast-462336" src="https://asciinema.org/a/462336.js" async></script>
+++


    - tikriname, ar naujas portas atsirado tos pačios „ankšties“ tinkliniame konteineryje `pause`:  +
      (per _Docker Port Mapping_ mechanizmą)
+++
<script id="asciicast-462338" src="https://asciinema.org/a/462338.js" async></script>
+++


==== Žingsnis 2.5: *dauginame konteinerius*

    - pakeliame „ankščių“ skaičių iki 3:
+++
<script id="asciicast-462340" src="https://asciinema.org/a/462340.js" async></script>
+++


    - tikriname „ankščių“ būsenas:
+
+++
<script id="asciicast-462341" src="https://asciinema.org/a/462341.js" async></script>
+++
Čia įrašą sustabdžiau kiek per anksti -- jis nespėja atspindėti pilnos išvesties, parodytos žemiau,
kur atsiranda dvi naujos `http-*` „ankštys“:
+
----
$ kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
http-774bb756bb-bbvm9         1/1     Running   0          43m
httpexposed-68cb8c8d4-d9b6w   1/1     Running   0          18m
$ 
$ kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
http-774bb756bb-bbvm9         1/1     Running   0          51m
http-774bb756bb-jcbgf         1/1     Running   0          7m50s
http-774bb756bb-qvqkc         1/1     Running   0          7m50s
httpexposed-68cb8c8d4-d9b6w   1/1     Running   0          26m
----


    - tikriname, ar „ankštys“ pateko į apkrovos balansavimą šiai paslaugai:
+++
<script id="asciicast-462343" src="https://asciinema.org/a/462343.js" async></script>
+++


    - atliekame kelias tos pačios paslaugos užklausas iš eilės:
+++
<script id="asciicast-462344" src="https://asciinema.org/a/462344.js" async></script>
+++


==== Suvestinė nr. 2:

    - Panaudojau `kubectl` komandas (ir subkomandas), ir:
    
      . startavau klasterį, įgalinau Kubectl CLI;
      . patikrinau mazgo būseną: veikia;
      . sukūriau įdiegimą su viena replika komandos `kubectl run ...` pagalba;
      . patikrinau HTTP paslaugos įdiegimo būseną: pradėjo veikti;
      . patikrinau išsamų įdiegimo aprašą: atitinka planą;
      . sukūriau paslaugą paviešindamas HTTP portą kaip `8000/TCP`;
      . patikrinau paslaugos veikimą: atsiliepia be klaidų;
      . sukūriau naują HTTP paslaugos diegimą kitu būdu -- iškart viešinant paslaugos portą;
      . šįkart HTTP portas yra `8001/TCP`;
      . patikrinau paslaugos veikimą: atsiliepia irgi;
      . patikrinau paslaugų sąrašą: naujojo porto nematyti;
      . patikrinau konteinerių sąrašą su Docker komanda:  +
        naujasis portas priklauso "k8s.gcr.io/pause" tipo konteineriui;
      . pakėliau pirmosios paslaugos „ankšties“ kopijų skaičių nuo 1 iki 3;
      . tikrinau jų būsenas ir sulaukiau, kol startuos dvi papildonos;
      . įsitikinau, kad visų trijų paslaugos „ankščių“ HTTP-portai pateko į apkrovos balansavimą;
      . atlikau šiai paslaugai keletą užklausų iš eilės:  +
        įsitikinau, kad atsako skirtingas Host ID (iš trijų galimų);
      . tyrimas baigtas.
    
    - `kubectl run --image= ...` komanda pyksta dėl _Deprecated_ opcijos `--generator`, nors aš tokios nenaudojau.  +
      Ir rekomenduoja naudoti vieną iš dviejų kitokių komandų.
+
=> Turbūt verta parašyti `katacoda` treniruoklio autoriams, kad atėjo metas atnaujinti instrukcijas. :)

    - Tikėtina, kad _Docker Port Mapping_ mechanizmas veikia būtent taip minima punkte nr. 12.  +
      Tačiau nežinau, kaip įsitikinti garantuotai, kad jis čia panaudotas.

    - Pasigedau veiksmo, kuriame būtume kurę replikavimo valdiklius, kaip žadėta užduoties aprašyme.    

<<<


[.text-left]
=== Užduotis nr. 3: "*https://www.katacoda.com/courses/kubernetes/creating-kubernetes-yaml-definitions[Deploy Containers Using YAML]*"

==== Įvadas

Mokinsimės Kubectl pagalba kurti ir startuoti įdiegimus, replikavimo valdiklius ir viešinti juos kaip paslaugas šįkart _jau_ pasinaudojant YAML apibrėžtimis (YAML formatu).

YAML apibrėžtimis aprašomi Kubernetes objektai, paskirti įdiegimams.
Taip pat bus ir galimybė keičiantis konfigūracijai šiuos objektus atnaujinti bei perdiegti į klasterį iš naujo.


==== Žingsnis 3.1: *įdiegimo kūrimas*

    - automatinis klasterio startas su _Shell_:
+
----
Your Interactive Bash Terminal. A safe place to learn and execute commands.

$ minikube start --wait=false
* minikube v1.8.1 on Ubuntu 18.04
* Using the none driver based on user configuration
* Running on localhost (CPUs=2, Memory=2460MB, Disk=145651MB) ...
* OS release is Ubuntu 18.04.4 LTS
* Preparing Kubernetes v1.17.3 on Docker 19.03.6 ...
  - kubelet.resolv-conf=/run/systemd/resolve/resolv.conf
* Launching Kubernetes ... 
* Enabling addons: default-storageclass, storage-provisioner
* Configuring local host environment ...
* Done! kubectl is now configured to use "minikube"
$ 
----


    - įkeliu YAML šabloną `deployment.yaml`:
+++
<script id="asciicast-462387" src="https://asciinema.org/a/462387.js" async></script>
+++


    - į klasterį diegiu aplikaciją `webapp1` iš Doker atvaizdo `katacoda/docker-http-server:latest`:
+++
<script id="asciicast-462388" src="https://asciinema.org/a/462388.js" async></script>
+++


    - peržiūriu įdiegimų sąrašą:
+++
<script id="asciicast-462389" src="https://asciinema.org/a/462389.js" async></script>
+++


    - peržiūriu `webapp1` įdiegimo aprašą:
+++
<script id="asciicast-462390" src="https://asciinema.org/a/462390.js" async></script>
+++


==== Žingsnis 3.2: *paslaugos kūrimas*

    - įkeliu YAML šabloną `service.yaml`:
+++
<script id="asciicast-462391" src="https://asciinema.org/a/462391.js" async></script>
+++


    - įdiegiu paslaugą:
+++
<script id="asciicast-462392" src="https://asciinema.org/a/462392.js" async></script>
+++


    - peržiūriu įdiegtų paslaugų sąrašą:
+++
<script id="asciicast-462393" src="https://asciinema.org/a/462393.js" async></script>
+++


    - peržiūriu paslaugos `webapp1-svc` aprašą:
+++
<script id="asciicast-462397" src="https://asciinema.org/a/462397.js" async></script>
+++


    - tikrinu paslaugos veikimą:
+++
<script id="asciicast-462398" src="https://asciinema.org/a/462398.js" async></script>
+++


==== Žingsnis 3.3: *įdiegimo dauginimas*

    - replikų (egzempliorių) skaičių YAML šablone `deployment.yaml` pakeliu iki 4:
+++
<script id="asciicast-462399" src="https://asciinema.org/a/462399.js" async></script>
+++


    - padauginu veikiančių replikų (egzempliorių) skaičių:
+++
<script id="asciicast-462400" src="https://asciinema.org/a/462400.js" async></script>
+++


    - tikrinu įdiegimo / klasterio būseną:
+++
<script id="asciicast-462402" src="https://asciinema.org/a/462402.js" async></script>
+++


    - tikrinu naujų „ankščių“ būseną:
+++
<script id="asciicast-462405" src="https://asciinema.org/a/462405.js" async></script>
+++


    - tikrinu užklausas į paslaugą:
+++
<script id="asciicast-462404" src="https://asciinema.org/a/462404.js" async></script>
+++


==== Suvestinė nr. 3:

    - Įvykdžiau diegimą pagal YAML šabloną (arba YAML apibrėžtį, angl. _definition_):
    
      . gavau Shell su veikiančiu K8s miniklasteriu;
      . įkėliau `deployment.yaml` šabloną;
      . pagal jį įdiegiau aplikaciją `webapp1` iš Docker atvaizdo `katacoda/docker-http-server`;
      . įsitikinau, kad įdiegimas pavyko;
      . peržiūrėjau jo aprašą, Host Port nenurodytas (`0/TCP`);
      . įkėliau `service.yaml` šabloną;
      . pagal jį įdiegiau HTTP paslaugą `webapp1-svc`;
      . įsitikinau, kad HTTP paslauga sukonfigūruota;
      . peržiūrėjau jos aprašą, `NodePort` prievadui priskirta `30080/TCP` reikšmė;
      . patikrinau paslaugos veikimą: ta pati „ankštis“ atsako į visas užklausas iš eilės;
      . padidinau replikų skaičių šablone `deployment.yaml` iki 4;
      . pritaikiau šabloną klasteriui su `kubectl apply` komanda;
      . patikrinau įdiegimo ir „ankščių“ būseną: skaičius pakilo iki 4;
      . patikrinau paslaugos veikimą: į užklausas atsako jau 4 skirtingos „ankštys“;
      . patikrinau paslaugos aprašą: yra visi 4 _Endpoints_  (iš jų vienas neišvestas dėl teksto trumpumo);  +
        (šito ataskaitoje neiliustravau)
      . tyrimas baigtas.

    - Naudojant YAML failus *tampa neaišku*:
    
      . kodėl atsiranda `app` raktažodis ? (Panašu, kad vietoj anksčiau naudoto `run`)
      . ką aprašo `spec.template` ir kas bus, jei **ne**nurodysiu `spec.template.metadata.labels.app` ?  
        (Kai jau tas pat nurodyta pas `spec.selector.app`)
      . kodėl įdiegimo apraše vardą `webapp1` reikia nurodyti net 4x:
        ** `spec.metadata.name` ?
        ** `spec.selector.matchLabels.app` ?
        ** `spec.template.metadata.labels.app` ?
        ** `spec.template.spec.containers.name` ?
      . kodėl TCP portą `80` reikia nurodyti tiek įdiegimui (`spec.template.spec.containers.ports`), tiek paslaugai (`spec.ports.port`), kai per CLI pakakdavo nurodyti tik vieną sykį ?
      . ar paslaugos `spec.selector.app` nurodo įdiegimo konteinerį, ar įdiegimo „ankštį“ (galvojant ne YAML scenarijaus sąvokomis) ?
      . Išvada: YAML šablonai įneša painavios į anksčiau susidarytą pradinį supratimą apie K8s.

    - Komanda `kubectl apply -f ...` pyksta:  +
      `Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply`  +
+
=> Galbūt irgi vertėtų pranešti treniruoklio autoriams (dėl instrukcijų patikslinimo).

<<<

[.text-left]
=== Užduotis nr. 4: "*https://www.katacoda.com/courses/kubernetes/guestbook[Deploy Guestbook example on Kubernetes]*"

==== Įvadas

Čia mokinsimės su Kubernetes ir Docker pagalba startuoti paprastą, bet daugiapakopę Web aplikaciją.
Siūlomos „Svečių knygos“ aplikacijos pavyzdys išsaugos puslapio svečių žinutes _Redis_ duomenų bazėje (DB) kviesdamas JavaScript API.
_Redis_ DB susideda iš _masterio_ (duomenų talpinimui) ir rinkinio iš replikuotų Redis _tarnų_.

Numatoma aprėpti tokias esmines sąvokas:

    - „ankštys“
    - replikavimo valdikliai
    - paslaugos
    - _NodePort_ prievadai

Jos sudaro _Kubernetes_ pagrindą.


==== Žingsnis 4.1: *klasterio startavimas*

    - _Shell_ ir automatinis vienamazgio klasterio startas:
+
----
Your Interactive Bash Terminal. A safe place to learn and execute commands.

controlplane $ mkdir -p /root/tutorial; cd /root/tutorial; launch.sh
Waiting for Kubernetes to start...
Kubernetes started
controlplane $ 
----


    - tikrinu klasterio būseną:
+
----
controlplane $ kubectl cluster-info
Kubernetes master is running at https://172.17.0.35:6443
KubeDNS is running at https://172.17.0.35:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

controlplane $ kubectl get nodes
NAME           STATUS     ROLES    AGE   VERSION
controlplane   NotReady   master   19s   v1.14.0

controlplane $ kubectl get nodes
NAME           STATUS   ROLES    AGE    VERSION
controlplane   Ready    master   2m6s   v1.14.0
node01         Ready    <none>   79s    v1.14.0
controlplane $ 
----


==== Žingsnis 4.2: *Redis _master_ valdiklis*

    - peržiūriu YAML aprašą:
+
----
controlplane $ cat redis-master-controller.yaml 
apiVersion: v1
kind: ReplicationController
metadata:
  name: redis-master
  labels:
    name: redis-master
spec:
  replicas: 1
  selector:
    name: redis-master
  template:
    metadata:
      labels:
        name: redis-master
    spec:
      containers:
      - name: master
        image: redis:3.0.7-alpine
        ports:
        - containerPort: 6379
controlplane $ 
----


    - sukuriu ir startuoju Redis _masterio_ replikacinį valdiklį:
+
----
controlplane $ kubectl create -f redis-master-controller.yaml 
replicationcontroller/redis-master created
controlplane $ 
----


    - tikrinu replikacinių valdiklių būseną:
+
----
controlplane $ kubectl get rc
NAME           DESIRED   CURRENT   READY   AGE
redis-master   1         1         0       5s

controlplane $ kubectl get rc
NAME           DESIRED   CURRENT   READY   AGE
redis-master   1         1         1       18s
controlplane $ 
----


    - tikrinu „ankščių“ / konteinerių būseną:
+
----
controlplane $ kubectl get pods
NAME                 READY   STATUS    RESTARTS   AGE
redis-master-bv75w   1/1     Running   0          4m1s
controlplane $ 
----


==== Žingsnis 4.3: *Redis _master_ paslauga*

    - peržiūriu YAML aprašą:
+
----
controlplane $ cat redis-master-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    name: redis-master
spec:
  ports:
    # the port that this service should serve on
  - port: 6379
    targetPort: 6379
  selector:
    name: redis-master
controlplane $ 
----


    - sukuriu ir startuoju Redis _masterio_ paslaugą:
+
----
controlplane $ kubectl create -f redis-master-service.yaml 
service/redis-master created
controlplane $ 
----


    - tikrinu paslaugos būseną:
+
----
controlplane $ kubectl get services
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes     ClusterIP   10.96.0.1      <none>        443/TCP    6m2s
redis-master   ClusterIP   10.98.222.56   <none>        6379/TCP   4s
controlplane $ 
----


    - peržiūriu Redis _masterio_ paslaugos aprašą:
+
----
controlplane $ kubectl describe services redis-master
Name:              redis-master
Namespace:         default
Labels:            name=redis-master
Annotations:       <none>
Selector:          name=redis-master
Type:              ClusterIP
IP:                10.98.222.56
Port:              <unset>  6379/TCP
TargetPort:        6379/TCP
Endpoints:         10.88.0.5:6379
Session Affinity:  None
Events:            <none>
controlplane $ 
----


==== Žingsnis 4.4: *Redis _tarnų_ valdiklis*

    - peržiūriu YAML aprašą:
+
----
controlplane $ cat redis-slave-controller.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: redis-slave
  labels:
    name: redis-slave
spec:
  replicas: 2
  selector:
    name: redis-slave
  template:
    metadata:
      labels:
        name: redis-slave
    spec:
      containers:
      - name: worker
        image: gcr.io/google_samples/gb-redisslave:v1
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below.
          # value: env
        ports:
        - containerPort: 6379
controlplane $ 
----


    - sukuriu ir startuoju Redis _tarnų_ replikacinį valdiklį:
+
----
controlplane $ kubectl create -f redis-slave-controller.yaml
replicationcontroller/redis-slave created
controlplane $ 
----


    - tikrinu replikacinių valdiklių būseną:
+
----
controlplane $ kubectl get rc
NAME           DESIRED   CURRENT   READY   AGE
redis-master   1         1         1       6m3s
redis-slave    2         2         0       3s

controlplane $ kubectl get rc
NAME           DESIRED   CURRENT   READY   AGE
redis-master   1         1         1       6m5s
redis-slave    2         2         2       5s
controlplane $ 
----


==== Žingsnis 4.5: *Redis _tarnų_ paslauga*

    - peržiūriu YAML aprašą:
+
----
controlplane $ cat redis-slave-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    name: redis-slave
spec:
  ports:
    # the port that this service should serve on
  - port: 6379
  selector:
    name: redis-slave
controlplane $ 
----


    - sukuriu ir startuoju Redis _tarnų_ paslaugą:
+
----
controlplane $ kubectl create -f redis-slave-service.yaml
service/redis-slave created
controlplane $ 
----


    - tikrinu paslaugos būseną:
+
----
controlplane $ kubectl get services
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes     ClusterIP   10.96.0.1      <none>        443/TCP    8m10s
redis-master   ClusterIP   10.98.222.56   <none>        6379/TCP   2m12s
redis-slave    ClusterIP   10.98.98.227   <none>        6379/TCP   1s
controlplane $ 
----


    - patikrinu ir „ankščių“ būsenas:
+
----
controlplane $ kubectl get pods
NAME                 READY   STATUS    RESTARTS   AGE
redis-master-bv75w   1/1     Running   0          7m23s
redis-slave-bfzm9    1/1     Running   0          83s
redis-slave-f5f9f    1/1     Running   0          83s
controlplane $ 
----


==== Žingsnis 4.6: *Frontendas -- replikacinis valdiklis ir jo „ankštys“*

    - peržiūriu YAML aprašą:
+
----
controlplane $ cat frontend-controller.yaml 
apiVersion: v1
kind: ReplicationController
metadata:
  name: frontend
  labels:
    name: frontend
spec:
  replicas: 3
  selector:
    name: frontend
  template:
    metadata:
      labels:
        name: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below.
          # value: env
        ports:
        - containerPort: 80
controlplane $ 
----


    - sukuriu ir startuoju replikacinį valdiklį pagal `gcr.io/google_samples/gb-frontend` atvaizdą:
+
----
controlplane $ kubectl create -f frontend-controller.yaml 
replicationcontroller/frontend created
controlplane $ 
----


    - tikrinu replikacinių valdiklių būsenas:
+
----
controlplane $ kubectl get rc
NAME           DESIRED   CURRENT   READY   AGE
frontend       3         3         3       88s
redis-master   1         1         1       12m
redis-slave    2         2         2       6m53s
controlplane $ kubectl get pods
----


    - tikrinu „ankščių“ būsenas:
+
----
controlplane $ kubectl get pods
NAME                 READY   STATUS    RESTARTS   AGE
frontend-ctpql       1/1     Running   0          93s
frontend-dwkqh       1/1     Running   0          93s
frontend-g998c       1/1     Running   0          93s
redis-master-bv75w   1/1     Running   0          12m
redis-slave-bfzm9    1/1     Running   0          6m58s
redis-slave-f5f9f    1/1     Running   0          6m58s
----


==== Žingsnis 4.7: *„Svečių knygos“ frontendinė paslauga*

    - peržiūriu YAML aprašą:
+
----
controlplane $ cat frontend-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    name: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  type: NodePort
  ports:
    # the port that this service should serve on
    - port: 80
      nodePort: 30080
  selector:
    name: frontend
controlplane $ 
----


    - sukuriu ir startuoju frontedinę paslaugą:
+
----
controlplane $ kubectl create -f frontend-service.yaml 
service/frontend created
controlplane $ 
----


    - tikrinu frontendinės paslaugos būseną:
+
----
controlplane $ kubectl get services
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
frontend       NodePort    10.96.81.216   <none>        80:30080/TCP   114s
kubernetes     ClusterIP   10.96.0.1      <none>        443/TCP        45m
redis-master   ClusterIP   10.98.222.56   <none>        6379/TCP       39m
redis-slave    ClusterIP   10.98.98.227   <none>        6379/TCP       37m
controlplane $ 
----


==== Žingsnis 4.8: *Jungiamės į „Svečių knygos“ frontendą*

    - tikrinu visų „ankščių“ būsenas (įsk. ir frontendines):
+
----
controlplane $ kubectl get pods
NAME                 READY   STATUS    RESTARTS   AGE
frontend-ctpql       1/1     Running   0          43m
frontend-dwkqh       1/1     Running   0          43m
frontend-g998c       1/1     Running   0          43m
redis-master-bv75w   1/1     Running   0          55m
redis-slave-bfzm9    1/1     Running   0          49m
redis-slave-f5f9f    1/1     Running   0          49m
controlplane $ 
----


    - išsifiltruoju frontendinės paslaugos _NodePort_ prievadą:
+
----
controlplane $ kubectl describe service frontend | grep NodePort:
NodePort:                 <unset>  30080/TCP
controlplane $ 
----


    - tikrinu Web-aplikaciją tiesiogiai (URL gautas iš treniruoklio puslapio):  +
      https://2886795348-30080-frugo01.environments.katacoda.com/
+
image::https://user-images.githubusercontent.com/74717106/149956447-fb106765-4990-4f70-8d05-24dc87f3c052.png[]
+
{nbsp}


==== Suvestinė nr. 4:

    - Įvykdžiau diegimą pagal YAML šabloną (arba YAML apibrėžtį, angl. _definition_):
    
      . gavau Shell su veikiančiu _K8s_ miniklasteriu;
      
      . taip pat gavau šešis YAML šablonus:
        ** `redis-master-controller.yaml`
        ** `redis-master-service.yaml`
        ** `redis-slave-controller.yaml`
        ** `redis-slave-service.yaml`
        ** `frontend-controller.yaml`
        ** `frontend-service.yaml`
        
      . pagal juos įdiegiau:
        ** `redis-master` replikacinį valdiklį iš Docker atvaizdo `redis:3.0.7-alpine`;
        ** `redis-master` paslaugą (1 vnt.);
        ** `redis-slave` replikacinį valdiklį iš Docker atvaizdo `gb-redisslave:v1`;
        ** `redis-slave` paslaugą (2 vnt.);
        ** `frontend` replikacinį valdiklį iš Docker atvaizdo `gb-frontend:v3`;
        ** `frontend` paslaugą (3 vnt.);
        
      . patikrinau būsenas:
        ** replikacinių valdiklių,
        ** paslaugų
        ** „ankščių“,
+
-- veikia 3 valdikliai, 3 paslaugos ir 6 „ankštys“.

      . patikrinau paslaugos veikimą iš išorinio interneto -- veikia puikiai (tik primityviai);
      
      . tyrimas baigtas.

    - Naudojant YAML failus man *tampa išvis neaišku*, kaip konfigūruoti moduliai sąveikauja žemame lygmenyje.  +
      Suvokiu tik abstraktų vaizdą, ir visiškai neaišku, kaip reikėtų tikrinti srautus / paslaugų strigimus įprastinėmis OS priemonėmis.


==== Laboratorinio darbo išvados

Minimaliai susipažinta su Kubernetes platforma.
